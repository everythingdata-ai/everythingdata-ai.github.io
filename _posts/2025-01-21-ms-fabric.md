---
layout: post
title: Microsoft Fabric DP-700 Certification (1/4)
categories: [Data Engineering, Azure]
---

Today I'm back on track with learning Microsoft Fabric.
This time through the official Microsoft learning path for the DP-700 certification.
Here are my notes from the first 3 chapters

## 1 - Getting started with lakehouses

Lakehouses merge data lake storage flexibility with data warehouse analytics. Microsoft Fabric offers a lakehouse solution for comprehensive analytics on a single SaaS platform.

#### Learning objectives

In this module, you learn how to:
- Describe core features and capabilities of lakehouses in Microsoft Fabric.
- Create a lakehouse.
- Ingest data into files and tables in a lakehouse.
- Query lakehouse tables with SQL.

### Explore the Microsoft Fabric lakehouse

A lakehouse presents as a database and is built on top of a data lake using Delta format tables. Lakehouses combine the SQL-based analytical capabilities of a relational data warehouse and the flexibility and scalability of a data lake. Lakehouses store all data formats and can be used with various analytics tools and programming languages. As cloud-based solutions, lakehouses can scale automatically and provide high availability and disaster recovery.

![Diagram of a lakehouse, displaying the folder structure of a data lake and the relational capabilities of a data warehouse.](https://learn.microsoft.com/en-gb/training/wwl/get-started-lakehouses/media/lakehouse-components.png)

The benefits of a lakehouse include :
- use of Spark and SQL engines to process large-scale data and support machine learning or predictive modeling analytics
- the data is organized in a schema-on-read format, which means you define the schema as needed rather than having a predefined schema
- support of ACID transactions through Delta Lake formatted tables for data consistency and integrity
- single location for data engineers, data scientists and data analysts

#### Load data into a lakehouse

MS Fabric allows the classic ETL process.

You can ingest data in many common formats from various sources, including local files, databases, APIs... You can also create shortcuts to data in external sources such as Azure Data Lake Store Gen2 or OneLake.

Ingested data can be transformed and then loaded using either Apache Spark (notebooks) or Dataflows Gen2.

You can use Data Factory pipelines to orchestrate your different ETL activities and land the prepared data into your lakehouse.

The lakehouse can be used to :
- Analyze using SQL
- Train machine learning models
- Perform analytics on real-time data
- Develop reports in Power BI

#### Secure a lakehouse. 

Lakehouse access is managed either through the workspace or item-level sharing.
Worspace roles should be used for collaborators because these roles grant access to all items within a workspace. Item-level sharing is best used for granting access for read-only needs, such as analytics or Power BI report development.

### Work with MSF lakehouses

#### Create and explore a lakehouse

When you create a new lakehouse, you have three different data items automatically created in your workspace.
- The **lakehouse** contains shortcuts, folders, files, and tables.
- The **Semantic model (default)** provides an easy data source for Power BI report developers.
- The **SQL analytics endpoint** allows read-only access to query data with SQL.

You can work with the data in the lakehouse in two modes:

- **lakehouse** enables you to add and interact with tables, files, and folders in the lakehouse.
- **SQL analytics endpoint** enables you to use SQL to query the tables in the lakehouse and manage its relational semantic model.
#### Ingest data into a lakehouse

Ingesting data into your lakehouse is the first step in your ETL process. Use any of the following methods to bring data into your lakehouse.

- **Upload**: Upload local files.
- **Dataflows Gen2**: Import and transform data using Power Query.
- **Notebooks**: Use Apache Spark to ingest, transform, and load data.
- **Data Factory pipelines**: Use the Copy data activity.

This data can then be loaded directly into files or tables. Consider your data loading pattern when ingesting data to determine if you should load all raw data as files before processing or use staging tables.

Spark job definitions can also be used to submit batch/streaming jobs to Spark clusters. By uploading the binary files from the compilation output of different languages (for example, .jar from Java), you can apply different transformation logic to the data hosted on a lakehouse. Besides the binary file, you can further customize the behavior of the job by uploading more libraries and command line arguments.

#### Access data using shortcuts

Another way to access and use data in Fabric is to use _shortcuts_. Shortcuts enable you to integrate data into your lakehouse while keeping it stored in external storage.

Shortcuts are useful when you need to source data that's in a different storage account or even a different cloud provider. Within your lakehouse you can create shortcuts that point to different storage accounts and other Fabric items like data warehouses, KQL databases, and other lakehouses.

Source data permissions and credentials are all managed by OneLake. When accessing data through a shortcut to another OneLake location, the identity of the calling user will be utilized to authorize access to the data in the target path of the shortcut. The user must have permissions in the target location to read the data.

Shortcuts can be created in both lakehouses and KQL databases, and appear as a folder in the lake. This allows Spark, SQL, Real-Time intelligence and Analysis Services to all utilize shortcuts when querying data.

### Explore and transform data in a lakehouse

#### Transform and load data

Most data requires transformations before loading into tables. You might ingest raw data directly into a lakehouse and then further transform and load into tables. Regardless of your ETL design, you can transform and load data simply using the same tools to ingest data. Transformed data can then be loaded as a file or a Delta table.

- Notebooks are favored by data engineers familiar with different programming languages including PySpark, SQL, and Scala.
- Dataflows Gen2 are excellent for developers familiar with Power BI or Excel since they use the PowerQuery interface.
- Pipelines provide a visual interface to perform and orchestrate ETL processes. Pipelines can be as simple or as complex as you need.

#### Analyze and visualize data in a lakehouse

After data is ingested, transformed, and loaded, it's ready for others to use. Fabric items provide the flexibility needed for every organization so you can use the tools that work for you.

- Data scientists can use notebooks or Data wrangler to explore and train machine learning models for AI.
- Report developers can use the semantic model to create Power BI reports.
- Analysts can use the SQL analytics endpoint to query, filter, aggregate, and otherwise explore data in lakehouse tables.

By combining the data visualization capabilities of Power BI with the centralized storage and tabular schema of a data lakehouse, you can implement an end-to-end analytics solution on a single platform.

### Summary

In this module, we explored how lakehouses fit into a data analytics solution using Microsoft Fabric. Lakehouses provide data engineers and analysts with the combined benefits of data lake storage and a relational data warehouse. You can use a lakehouse as the basis of an end-to-end data analytics solution that includes data ingestion, transformation, modeling, and visualization.

Fabric lakehouses provide value as a Software-as-a-Service data store that provides all of the benefits with less administration.

## 2 - Using Apache Spark

Apache Spark is a core technology for large-scale data analytics. Microsoft Fabric provides support for Spark clusters, enabling you to analyze and process data in a Lakehouse at scale.

#### Learning objectives

In this module, you learn how to:
- Configure Spark in a Microsoft Fabric workspace
- Identify suitable scenarios for Spark notebooks and Spark jobs
- Use Spark dataframes to analyze and transform data
- Use Spark SQL to query data in tables and views
- Visualize data in a Spark notebook

#### Prepare to use Spark

Apache Spark is a distributed data processing framework that enables large-scale data analytics by coordinating work across multiple processing nodes in a cluster, known in Microsoft Fabric as a _Spark pool_. Put more simply, Spark uses a "divide and conquer" approach to processing large volumes of data quickly by distributing the work across multiple computers. The process of distributing tasks and collating results is handled for you by Spark.

Spark can run code written in a wide range of languages, including Java, Scala (a Java-based scripting language), Spark R, Spark SQL, and PySpark (a Spark-specific variant of Python). In practice, most data engineering and analytics workloads are accomplished using a combination of PySpark and Spark SQL.

A Spark pool consists of compute nodes that distribute data processing tasks, and contains two kinds of node:
1. A _head_ node in a Spark pool coordinates distributed processes through a _driver_ program.
2. The pool includes multiple _worker_ nodes on which _executor_ processes perform the actual data processing tasks.

Microsoft Fabric provides a _starter pool_ in each workspace, enabling Spark jobs to be started and run quickly with minimal setup and configuration.
You can manage settings for the starter pool and create new Spark pools in the Data Engineering/Science section of the workspace settings.

#### Environments in Microsoft Fabric

Microsoft Fabric supports multiple Spark runtimes, and will continue to add support for new runtimes as they are released. You can use the workspace settings interface to specify the Spark runtime that is used by default environment when a Spark pool is started.

You can also create custom environments in a Fabric workspace, enabling you to use specific Spark runtimes, libraries, and configuration settings for different data processing operations.

When creating an environment, you can:
- Specify the Spark runtime it should use.
- View the built-in libraries that are installed in every environment.
- Install specific public libraries from the Python Package Index (PyPI).
- Install custom libraries by uploading a package file.
- Specify the Spark pool that the environment should use.
- Specify Spark configuration properties to override default behavior.
- Upload resource files that need to be available in the environment.

#### Additional Spark configuration options

- Native execution engine : a vectorized processing engine that runs Spark operations directly on lakehouse infrastructure. Using the native execution engine can significantly improve the performance of queries when working with large data sets in Parquet or Delta file formats.

To enable the native execution engine for a specific script or notebook, you can set these configuration properties at the beginning of your code, like this:

```JSON
%%configure 
{ 
	"conf": { 
		"spark.native.enabled": "true", 		
		"spark.shuffle.manager": "org.apache.spark.shuffle.sort.ColumnarShuffleManager" 
	} 
}
```

- High concurrency mode : share Spark sessions across multiple concurrent users or processes. When high concurrency mode is enabled for Notebooks, multiple users can run code in notebooks that use the same Spark session, while ensuring isolation of code to avoid variables in one notebook being affected by code in another notebook.
To enable high concurrency mode, use the Data Engineering/Science section of the workspace settings interface.

- Automatic MLFlow logging : MLFlow is an open source library that is used in data science workloads to manage machine learning training and model deployment. A key capability of MLFlow is the ability to log model training and management operations. By default, Microsoft Fabric uses MLFlow to implicitly log machine learning experiment activity without requiring the data scientist to include explicit code to do so. You can disable this functionality in the workspace settings.

- Spark administration for a Fabric capacity : Administrators can manage Spark settings at a Fabric capacity level, enabling them to restrict and override Spark settings in workspaces within an organization.

#### Run Spark code

To edit and run Spark code you can use :

- A notebook : to explore and analyze data interactively, use a notebook. Notebooks enable you to combine text, images, and code written in multiple languages to create an interactive item that you can share with others and collaborate. They consist of one or more cells, each of which can contain markdown-formatted content or executable code. You can run the code interactively in the notebook and see the results immediately.


- A Spark job : to use Spark to ingest and transform data as part of an automated process, you can define a Spark job to run a script on-demand or based on a schedule. To configure a Spark job, create a Spark Job Definition in your workspace and specify the script it should run. You can also specify a reference file (for example, a Python code file containing definitions of functions that are used in your script) and a reference to a specific lakehouse containing data that the script processes.
#### Work with data in a Spark dataframe 

Natively, Spark uses a data structure called a _resilient distributed dataset_ (RDD); but while you _can_ write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the _dataframe_, which is provided as part of the _Spark SQL_ library. Dataframes in Spark are similar to those in the ubiquitous _Pandas_ Python library, but optimized to work in Spark's distributed processing environment.

In a Spark notebook, you could use the following PySpark code to load the file data into a dataframe and display the first 10 rows:

```python
%%pyspark 
df = spark.read.load('Files/data/products.csv', 
					 format='csv', 
					 header=True ) 
display(df.limit(10))
```

The `%%pyspark` line at the beginning is called a _magic_, and tells Spark that the language used in this cell is PySpark. You can select the language you want to use as a default in the toolbar of the Notebook interface, and then use a magic to override that choice for a specific cell. For example, here's the equivalent Scala code for the products data example:

```scala
%%spark 
val df = spark.read.format("csv").option("header","true").load("Files/data/products.csv") 
display(df.limit(10))
```

The magic `%%spark` is used to specify Scala.

In the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file

```python
from pyspark.sql.types import * 
from pyspark.sql.functions import * 

productSchema = StructType([ 
	StructField("ProductID", IntegerType()), 
	StructField("ProductName", StringType()), 
	StructField("Category", StringType()), 
	StructField("ListPrice", FloatType()) 
]) 

df = spark.read.load('Files/data/product-data.csv', 
	format='csv', 
	schema=productSchema, 
	header=False) 
	
display(df.limit(10))
```

You can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the select method to retrieve the ProductID and ListPrice columns from the df dataframe containing product data in the previous example:

```python
pricelist_df = df.select("ProductID", "ListPrice")

#  can also be achieved by using the following shorter syntax
pricelist_df = df["ProductID", "ListPrice"]
```

You can "chain" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes:

```python
bikes_df = df.select("ProductName", "Category", "ListPrice").where((df["Category"]=="Mountain Bikes") | (df["Category"]=="Road Bikes")) 
display(bikes_df)
```

To group and aggregate data, you can use the groupBy method and aggregate functions. For example, the following PySpark code counts the number of products for each category:

```python
counts_df = df.select("ProductID", "Category").groupBy("Category").count()
display(counts_df)
```

You'll often want to use Spark to transform raw data and save the results for further analysis or downstream processing. The following code example saves the dataFrame into a _parquet_ file in the data lake, replacing any existing file of the same name :

```python
bikes_df.write.mode("overwrite").parquet('Files/product_data/bikes.parquet')
```

Partitioning is an optimization technique that enables Spark to maximize performance across the worker nodes. More performance gains can be achieved when filtering data in queries by eliminating unnecessary disk IO.

To save a dataframe as a partitioned set of files, use the **partitionBy** method when writing the data. The following example saves the bikes_df dataframe (which contains the product data for the _mountain bikes_ and _road bikes_ categories), and partitions the data by category:

```python
bikes_df.write.partitionBy("Category").mode("overwrite").parquet("Files/bike_data")
```

The folder names generated when partitioning a dataframe include the partitioning column name and value in a column=value format, so the code example creates a folder named bike_data that contains the following subfolders: 
- Category=Mountain Bikes 
- Category=Road Bikes 
Each subfolder contains one or more parquet files with the product data for the appropriate category.

When reading partitioned data into a dataframe, you can load data from any folder within the hierarchy by specifying explicit values or wildcards for the partitioned fields. The following example loads data for products in the Road Bikes category:

```python
road_bikes_df = spark.read.parquet('Files/bike_data/Category=Road Bikes')
display(road_bikes_df.limit(5))
```

#### Work with data using Spark SQL

The Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers.

One of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view :

```python
df.createOrReplaceTempView("products_view")
```

A view is temporary, meaning that it's automatically deleted at the end of the current session. You can also create _tables_ that are persisted in the catalog to define a database that can be queried using Spark SQL :

```python
spark.catalog.createTable("test")
df.write.format("delta").saveAsTable("products")
```

You can use the Spark SQL API in code written in any language to query data in the catalog. For example, the following PySpark code uses a SQL query to return data from the products table as a dataframe :

```python
bikes_df = spark.sql("SELECT ProductID, ProductName, ListPrice \
                      FROM products \
                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')")
display(bikes_df)
```

 You can also use the `%%sql` magic to run SQL code that queries objects in the catalog :

```sql
%%sql

SELECT Category, COUNT(ProductID) AS ProductCount
FROM products
GROUP BY Category
ORDER BY Category
```


#### Visualize data in a Spark notebook

One of the most intuitive ways to analyze the results of data queries is to visualize them as charts. Notebooks in Microsoft Fabric provide some basic charting capabilities in the user interface, and when that functionality doesn't provide what you need, you can use one of the many Python graphics libraries to create and display data visualizations in the notebook.

When you display a dataframe or run a SQL query in a Spark notebook, the results are displayed under the code cell. By default, results are rendered as a table, but you can also change the results view to a chart and use the chart properties to customize how the chart visualizes the data.

Here's an example of the Matplotlib library to create a chart :

```python
from matplotlib import pyplot as plt

# Get the data as a Pandas dataframe
data = spark.sql("SELECT Category, COUNT(ProductID) AS ProductCount \
                  FROM products \
                  GROUP BY Category \
                  ORDER BY Category").toPandas()

# Clear the plot area
plt.clf()

# Create a Figure
fig = plt.figure(figsize=(12,8))

# Create a bar plot of product counts by category
plt.bar(x=data['Category'], height=data['ProductCount'], color='orange')

# Customize the chart
plt.title('Product Counts by Category')
plt.xlabel('Category')
plt.ylabel('Products')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=70)

# Show the plot area
plt.show()
```

## 3 - Working with Delta Lake tables

Tables in a Microsoft Fabric lakehouse are based on the Delta Lake storage format commonly used in Apache Spark. By using the enhanced capabilities of delta tables, you can create advanced analytics solutions.

Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data. By using Delta Lake, you can implement a lakehouse architecture to support SQL-based data manipulation semantics in Spark with support for transactions and schema enforcement. The result is an analytical data store that offers many of the advantages of a relational database system with the flexibility of data file storage in a data lake.

While you don't need to work directly with Delta Lake APIs in order to use tables in a Fabric lakehouse, an understanding of the Delta Lake metastore architecture and familiarity with some of the more specialized Delta table operations can greatly expand your ability to build advanced analytics solutions on Microsoft Fabric.

#### Learning objectives

In this module, you learn how to:
- Understand Delta Lake and delta tables
- Create and manage delta tables using Spark
- Optimize delta tables
- Query and transform delta tables 

### Understand Delta Lake

Delta Lake is an open-source storage layer that adds relational database semantics to Spark-based data lake processing. Tables in Microsoft Fabric lakehouses are Delta tables, which is signified by the triangular Delta (**Δ**) icon on tables in the lakehouse user interface.

Delta tables are schema abstractions over data files that are stored in Delta format. For each table, the lakehouse stores a folder containing Parquet data files and a _ delta _ Log folder in which transaction details are logged in JSON format.

The benefits of using Delta tables include:

- **Relational tables that support querying and data modification**. With Apache Spark, you can store data in Delta tables that support _CRUD_ (create, read, update, and delete) operations. In other words, you can _select_, _insert_, _update_, and _delete_ rows of data in the same way you would in a relational database system.

- **Support for _ACID_ transactions**. Relational databases are designed to support transactional data modifications that provide _atomicity_ (transactions complete as a single unit of work), _consistency_ (transactions leave the database in a consistent state), _isolation_ (in-process transactions can't interfere with one another), and _durability_ (when a transaction completes, the changes it made are persisted). Delta Lake brings this same transactional support to Spark by implementing a transaction log and enforcing serializable isolation for concurrent operations.

- **Data versioning and _time travel_**. Because all transactions are logged in the transaction log, you can track multiple versions of each table row and even use the _time travel_ feature to retrieve a previous version of a row in a query.

- **Support for batch and streaming data**. While most relational databases include tables that store static data, Spark includes native support for streaming data through the Spark Structured Streaming API. Delta Lake tables can be used as both _sinks_ (destinations) and _sources_ for streaming data.

- **Standard formats and interoperability**. The underlying data for Delta tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines. Additionally, you can use the SQL analytics endpoint for the Microsoft Fabric lakehouse to query Delta tables in SQL.

#### Create delta tables

When you create a table in a MS Fabric lakehouse, a delta table is defined in the metastore and the data is stored in a Parquet file, however this is all abstracted. Spark allows you to have greater control over delta tables.

You can create a delta table from a datafram :

```python
# Load a file into a dataframe
df = spark.read.load('Files/mydata.csv', format='csv', header=True)

# Save the dataframe as a delta table
df.write.format("delta").saveAsTable("mytable")
```
The data for the table is saved in Parquet files, along with a delta_log folder containing the transaction logs for the table.
In this case, the dataframe was saved as a managed table, meaning Spark took care of everything.

If however you want to create external tables, in which the relational table definition in the metastore is mapped to an alternative file storage location, you can use the code below :

```python
df.write.format("delta").saveAsTable("myexternaltable", path="Files/myexternaltable")

# You can also specify a fully qualified path
df.write.format("delta").saveAsTable("myexternaltable", path="abfss://my_store_url..../myexternaltable")
```

#### Create table metadata

While in most cases a table will be created from existing data in a dataframe, there are scnarios where you want to create a table definition first that will be populated with data in other ways.
You can do that in different ways :

- Using the DeltaTableBuilder API :
```python
from delta.tables import *

DeltaTable.create(spark) \
  .tableName("products") \
  .addColumn("Productid", "INT") \
  .addColumn("ProductName", "STRING") \
  .addColumn("Category", "STRING") \
  .addColumn("Price", "FLOAT") \
  .execute()
```

- Using Spark SQL for a managed table :

```python
%%sql

CREATE TABLE salesorders
(
    Orderid INT NOT NULL,
    OrderDate TIMESTAMP NOT NULL,
    CustomerName STRING,
    SalesTotal FLOAT NOT NULL
)
USING DELTA
```

- Using Spark SQL for an external table :

```python
%%sql

CREATE TABLE MyExternalTable
USING DELTA
LOCATION 'Files/mydata'
```

In this case the schema of the table is determined by the Parquet files in the specified location.
This approach can be useful when you want to create a table definition that references data that has already been saved in delta format, or based on a folder where you expect to ingest data in delta format.

#### Saving data in delta format

Apart from saving a dataframe as a delta lake, and creating the table definition, a third possibility is to save data in delta format without creating a table definition.
This approach can be useful when you want to persist the results of data transformations performed in Spark in a file format over which you can later "overlay" a table definition or process directly by using the delta lake API.

This code saves a dataframe to a new folder location in delta format:

```python
delta_path = "Files/mydatatable"
df.write.format("delta").save(delta_path)

# You can replace the contents of an existing folder with the data in a dataframe by using the overwrite mode
new_df.write.format("delta").mode("overwrite").save(delta_path)

# Or you can add rows from a dataframe to an existing folder by using the append mode
new_rows_df.write.format("delta").mode("append").save(delta_path)
```

### Optimize delta tables

Spark is a parallel-processing framework, with data stored in one or more worker nodes.
Parquet files are immutable with new files written for every update or delete. This process can result in Spark storing data in a large number of small files, known as the small file problem. It means that queries over large amounts of data can run slowly, and in some cases, fail to complete.

#### OptimizeWrite function

OptimizeWrite is a feature of Delta Lake which reduces the number of files as they're written. Instead of writing many small files, it writes fewer larger files. This helps to prevent the small files problem and ensure that performance isn't degraded.

![Diagram showing how Optimize Write writes fewer large files.](https://learn.microsoft.com/en-gb/training/wwl/work-delta-lake-tables-fabric/media/optimize-write.png)

It is enabled by default in MS Fabric, and can be disabled at the session level :

```python
# Disable Optimize Write at the Spark session level
spark.conf.set("spark.microsoft.delta.optimizeWrite.enabled", False)

# Enable Optimize Write at the Spark session level
spark.conf.set("spark.microsoft.delta.optimizeWrite.enabled", True)

print(spark.conf.get("spark.microsoft.delta.optimizeWrite.enabled"))
```

#### Optimize

Optimize is a table maintenance feature that consolidates small Parquet files into fewer large files.
You might run it after loading large tables, resulting in :
- fewer larger files
- better compression
- efficient data distribution across nodes

![Diagram showing how Optimize consolidates Parquet files.](https://learn.microsoft.com/en-gb/training/wwl/work-delta-lake-tables-fabric/media/optimize-command.png)

To run Optimize: 
- In Lakehouse Explorer, select the ... menu beside a table name and select Maintenance
- Select Run OPTIMIZE command
- Optionally, select Apply V-order to maximize reading speeds in Fabric
- Select Run now

### V-Oder function

V-Order  is designed for the Parquet file format in Fabric, and enables lightning-fast reads, with in-memory-like data access times. It also improves cost efficiency as it reduces network, disk, and CPU resources during reads.

It is enabled by default in Microsoft Fabric and is applied as data is being written. It incurs a small overhead of about 15% making writes a little slower. However, V-Order enables faster reads from the Microsoft Fabric compute engines, such as Power BI and SQL engines(using VertiScan technology), Spark, and others.

V-Order works by applying special sorting, row group distribution, dictionary encoding, and compression on Parquet files. It's 100% compliant to the open-source Parquet format and all Parquet engines can read it.

V-Order might not be beneficial for write-intensive scenarios such as staging data stores where data is only read once or twice. In these situations, disabling V-Order might reduce the overall processing time for data ingestion.

#### Vacumm

The VACUUM command enables you to remove old data files.

Every time an update or delete is done, a new Parquet file is created and an entry is made in the transaction log. Old Parquet files are retained to enable time travel, which means that Parquet files accumulate over time.

The VACUUM command removes old Parquet data files, but not the transaction logs. When you run VACUUM, you can't time travel back earlier than the retention period.

![Diagram showing how vacuum works.](https://learn.microsoft.com/en-gb/training/wwl/work-delta-lake-tables-fabric/media/how-vacuum-works.png)

Data files that aren't currently referenced in a transaction log and that are older than the specified retention period are permanently deleted by running VACUUM. Choose your retention period based on factors such as:

- Data retention requirements
- Data size and storage costs
- Data change frequency
- Regulatory requirements

The default retention period is 7 days (168 hours), and the system prevents you from using a shorter retention period.

You can run VACUUM on an ad-hoc basis or scheduled using Fabric notebooks.

Run VACUUM on individual tables by using the Table maintenance feature:

1. In **Lakehouse Explorer**, select the ... menu beside a table name and select **Maintenance**.
2. Select **Run VACUUM command using retention threshold** and set the retention threshold.
3. Select **Run now**.

You can also run VACUUM as a SQL command in a notebook:

```python
%%sql 
VACUUM lakehouse2.products RETAIN 168 HOURS;

DESCRIBE HISTORY lakehouse2.products;
```

#### Partitioning Delta tables

Delta Lake allows you to organize data into partitions. This might improve performance by enabling data skipping, which boosts performance by skipping over irrelevant data objects based on an object's metadata.

Consider a situation where large amounts of sales data are being stored. You could partition sales data by year. The partitions are stored in subfolders named "year=2021", "year=2022", etc. If you only want to report on sales data for 2024, then the partitions for other years can be skipped, which improves read performance.

Partitioning of small amounts of data can degrade performance, however, because it increases the number of files and can exacerbate the "small files problem."

Use partitioning when:

- You have very large amounts of data.
- Tables can be split into a few large partitions.

Don't use partitioning when:

- Data volumes are small.
- A partitioning column has high cardinality, as this creates a large number of partitions.
- A partitioning column would result in multiple levels.

![Diagram showing partitioning by one or more columns.](https://learn.microsoft.com/en-gb/training/wwl/work-delta-lake-tables-fabric/media/partitioning.png)

Partitions are a fixed data layout and don't adapt to different query patterns. When considering how to use partitioning, think about how your data is used, and its granularity.

In this example, a DataFrame containing product data is partitioned by Category:

```python
df.write.format("delta").partitionBy("Category").saveAsTable("partitioned_products", path="abfs_path/partitioned_products")
```


We can create a similar partitioned table using SQL :

```sql
%%sql
CREATE TABLE partitioned_products (
    ProductID INTEGER,
    ProductName STRING,
    Category STRING,
    ListPrice DOUBLE
)
PARTITIONED BY (Category);
```

### Work with delta tables in Spark

You can work with delta tables (or delta format files) to retrieve and modify data in multiple ways.

#### Using Spark SQL

The most common way to work with data in delta tables in Spark is to use Spark SQL. You can embed SQL statements in other languages (such as PySpark or Scala) by using the spark.sql library. For example, the following code inserts a row into the products table.

```python
spark.sql("INSERT INTO products VALUES (1, 'Widget', 'Accessories', 2.99)")
```

Alternatively, you can use the `%%sql` magic in a notebook to run SQL statements.

```sql
%%sql

UPDATE products
SET Price = 2.49 WHERE ProductId = 1;
```

#### Using the Delta API

When you want to work with delta files rather than catalog tables, it may be simpler to use the Delta Lake API. You can create an instance of a DeltaTable from a folder location containing files in delta format, and then use the API to modify the data in the table.

```python
from delta.tables import *
from pyspark.sql.functions import *

# Create a DeltaTable object
delta_path = "Files/mytable"
deltaTable = DeltaTable.forPath(spark, delta_path)

# Update the table (reduce price of accessories by 10%)
deltaTable.update(
    condition = "Category == 'Accessories'",
    set = { "Price": "Price * 0.9" })
```

#### Using time travel to work with table versioning

Modifications made to delta tables are logged in the transaction log. You can view the history of changes and retrieve older versions of the data (time travel)

To see the history of a table :

```mysql
%%sql 
DESCRIBE HISTORY products

## For an external table
DESCRIBE HISTORY 'Files/mytable'
```

You can retrieve data from a specific version of the data with the versionAsOf option, or by specifying a timestampAsOf :

```python

# Option 1
df = spark.read.format("delta").option("versionAsOf", 0).load(delta_path)

# Option 2
df = spark.read.format("delta").option("timestampAsOf", '2022-01-01').load(delta_path)
```

### Use delta tables with streaming data

All of the data explored so far gas been static in files. However, many scenarios involve streaming data that must be processed in near real time, for example IoT devices. Spark processes batch and streaming data in the same way.

#### Spark Structured Streaming

A typical stream processing solution involves:

- Constantly reading a stream of data from a _source_.
- Optionally, processing the data to select specific fields, aggregate and group values, or otherwise manipulating the data.
- Writing the results to a _sink_.

Spark includes native support for streaming data through _Spark Structured Streaming_, an API that is based on a boundless dataframe in which streaming data is captured for processing. A Spark Structured Streaming dataframe can read data from many different kinds of streaming source, including:

- Network ports
- Real time message brokering services such as Azure Event Hubs or Kafka
- File system locations.

#### Streaming with Delta tables

You can use a Delta table as a source of sink for SSS. For example, capturing real time data from an IoT device and writing it to a Delta table as a sink. Or you could read a Delta as a streaming source, enabling near real-time reporting as new data is added to the table.


In the following PySpark example, a Delta table is created to store details of Internet sales orders:

```mysql
%%sql
CREATE TABLE orders_in
(
        OrderID INT,
        OrderDate DATE,
        Customer STRING,
        Product STRING,
        Quantity INT,
        Price DECIMAL
)
USING DELTA;

# Hypothetical data sctream of internet orders
INSERT INTO orders_in (OrderID, OrderDate, Customer, Product, Quantity, Price) VALUES 
	(3001, '2024-09-01', 'Yang', 'Road Bike Red', 1, 1200), 
	(3002, '2024-09-01', 'Carlson', 'Mountain Bike Silver', 1, 1500), 
	(3003, '2024-09-02', 'Wilson', 'Road Bike Yellow', 2, 1350), 
	(3004, '2024-09-02', 'Yang', 'Road Front Wheel', 1, 115), 
	(3005, '2024-09-02', 'Rai', 'Mountain Bike Black', 1, NULL);
```

```python
# Read and display the input table
df = spark.read.format("delta").table("orders_in")

display(df)
```
The data is then loaded into a streaming DataFrame from the Delta table:

```python
# Load a streaming DataFrame from the Delta table
stream_df = spark.readStream.format("delta") \
    .option("ignoreChanges", "true") \
    .table("orders_in")

# Verify that the stream is streaming stream_df.isStreaming
stream_df.isStreaming
```

After reading the data from the Delta table into a streaming DataFrame, you can use the Spark Structured Streaming API to process it. For example, you could count the number of orders placed every minute and send the aggregated results to a downstream process for near-real-time visualization.

In this example, any rows with NULL in the Price column are filtered and new columns are added for IsBike and Total.

```python
from pyspark.sql.functions import col, expr

transformed_df = stream_df.filter(col("Price").isNotNull()) \
    .withColumn('IsBike', expr("INSTR(Product, 'Bike') > 0").cast('int')) \
    .withColumn('Total', expr("Quantity * Price").cast('decimal'))
```

The data stream is then written to a Delta table :

```python
# Write the stream to a delta table
output_table_path = 'Tables/orders_processed'
checkpointpath = 'Files/delta/checkpoint'
deltastream = transformed_df.writeStream.format("delta").option("checkpointLocation", checkpointpath).start(output_table_path)

print("Streaming to orders_processed...")
```
