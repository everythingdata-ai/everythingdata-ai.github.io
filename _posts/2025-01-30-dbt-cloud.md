---
layout: post
title: Quickstart for dbt Cloud and BigQuery
categories: [Data Engineering, GCP, SQL]
---

Almost every Data Engineering job offer these days has dbt listed.

It was about time I gave it a try.

Today I will be doing a quick tutorial to set up a dbt Cloud project on a BigQuery dataset.

### Goals
- Connect dbt Cloud to BigQuery
- Turn a simple query into a model
- Add tests
- Document your models
- Schedule a job to run

### Pre-requisites
- A [Google Cloud Platform](https://cloud.google.com/free) account
- A [dbt Cloud](https://www.getdbt.com/signup/) account

### What is dbt ?
First, if you're like me, seeing dbt pop-up everywhere and not knowing what it is, here is a quick instroduction.

The formal definition : dbt is an open-source data transformation tool that enables analysts and engineers to transform raw data into meaningful insights by writing modular SQL queries, version-controlling them, and automating their execution.

The simple definition : dbt just executes SQL using the already existing SQL engine in the database.

![dbt](/images/posts/2025/01/dbt.png)

### Setting up BigQuery
First go to the [BigQuery Console](https://console.cloud.google.com/bigquery)  and create a new project. Let's call it "dbt learn"
Then create two new datasets with these values :
- Dataset ID : one for jaffle_shop and another one for stripe afterward
- Data Location : leave it blank or select the same location where your data is stored
- Advanced options : Allow Google to manage encryption
Datasets in BigQuery are equivalent to schemas in a traditional database.

Next we need to generate BigQuery credentials :
Start the [GCP credentials wizard](https://console.cloud.google.com/apis/credentials/wizard), choose BigQuery API in the dropdown menu and Application data for the type of data you will be accessing.
Click next to create a service account with these values :
- Service account name : dbt-user
- Roles : BigQuery Job user and BigQuery Data Editor

Finally, create a service account key for the project on the Keys tab, choose JSON as the key type and download your file. This file is sensitive and should be protected, don't include it in the Git project.


### Connect dbt Cloud to BigQuery
Create a new project in dbt Cloud from the Account Settings. If you're using the trial version, a project will have already been created for you.
For the connection, select BigQuery and upload the previously created Service Account JSON file.
Click Test Connection to see if it was established, and if it succeeeds click Next.
You can link your project to a Git project, but for this tutorial let's use the "Managed" option, the version control will be managed directly in dbt Cloud.
Click "Start developing in the IDE", and then "Initialize dbt project".
You can make your initial commit by clicking Commit and Sync, this will allow you to open a branch where you will add new dbt code.

And we're all set, our BigQuery project is now connected to dbt Cloud, and you can directly query data from the warehouse.

### Build your first model
dbt Cloud doesn't allow commits to the protected main branch, so let's create a new branch on the Version control tab, and call it my-first-model.

Create a new file in the model folder called test.sql and add this query to it :

```sql
select * from `dbt-tutorial.jaffle_shop.customers`
```
After saving the file, enter `dbt run` in the command line bar at the bottom of the screen and click enter, if everything goes well, and it should go well, you will see "dbt run succeeded" message.


Now that we made sure everything is working fine, let's create our first model.
Create another file in the models directory called customers.sql using the sql query bellow :

```sql
with customers as (
    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.jaffle_shop.customers
),

orders as (
    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.jaffle_shop.orders
),

customer_orders as (
    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1
),

final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)
)

select * from final
```

Enter `dbt run` in the command prompt, and once it's done you can refresh your BigQuery project to make sure the model was created.

### Change the way your model is materialized
By default, dbt models get created as Views, you can override that either at :
- The directory level : this will change the materialization to everything in the directory. To do that you need to edit the dbt_project.yml file :
```yaml
name: 'jaffle_shop'

models:
  jaffle_shop:
    +materialized: table
    example:
    +materialized: view
```

Once you save this file, everything in jaffle_shop will be materialized as a table, and everything in example will be materialized as a view.

- The model level : you can override the dbt_project.yml setting by adding the following snippet at the top of your model :

```sql
{{  
	config(  
		materialized='view'  
	)  
}}  
  
with customers as (  
	select  
		id as customer_id  
...  

```

 You can run the `dbt run --full-refresh` command to fully apply materialization changes on BigQuery.
 
 You can now delete the previously created test.sql file as well as the example folder and its configuration on the dbt_project.yml file.
### Build models on top of models 
As a best practice in SQL, you should separate logic that cleans up your data from logic that transforms your data. We did that in the customers query by using CTEs, but let's use the ref function to separate the logic into different models.

Create a file called models/stg_customers.sql containing the SQL from the customers CTE in the original query, and another one called models/stg_orders.sql with the oders CTE

Edit the models/customers.sql as follows : 

```sql
with customers as (
    select * from {{ ref('stg_customers') }}
),

orders as (
    select * from {{ ref('stg_orders') }}
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders
    from orders
    group by 1
),

final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders
    from customers
    left join customer_orders using (customer_id)
)

select * from final
```

Now when you execute `dbt run`, separate views/tables will be created for stg_customers, stg_orders and customers. dbt automatically inferes the order to run these models, since customers depends on the other two, it will be run last.

### Add tests to your models
You can add test to your models to help validate that they're working correctly.
To do that, create a new YAML file in the models directory, named models/schema.yml, and add the following contents :

```yaml
version: 2

models:
  - name: customers
    columns:
      - name: customer_id
        tests:
          - unique
          - not_null

  - name: stg_customers
    columns:
      - name: customer_id
        tests:
          - unique
          - not_null

  - name: stg_orders
    columns:
      - name: order_id
        tests:
          - unique
          - not_null
      - name: status
        tests:
          - accepted_values:
              values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']
      - name: customer_id
        tests:
          - not_null
          - relationships:
              to: ref('stg_customers')
              field: customer_id

```

Run `dbt test` to see if all the tests passed.

### Document your model

A part from testing, you can also document your models, describing them in rich detail and sharing that information with your team.
You can add documentation to your models in the same models/schema.yml file used for testing, let's update it :

```yaml
version: 2

models:
  - name: customers
    description: One record per customer
    columns:
      - name: customer_id
        description: Primary key
        tests:
          - unique
          - not_null
      - name: first_order_date
        description: NULL when a customer has not yet placed an order.

  - name: stg_customers
    description: This model cleans up customer data
    columns:
      - name: customer_id
        description: Primary key
        tests:
          - unique
          - not_null

  - name: stg_orders
    description: This model cleans up order data
    columns:
      - name: order_id
        description: Primary key
        tests:
          - unique
          - not_null
      - name: status
        tests:
          - accepted_values:
              values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']
      - name: customer_id
        tests:
          - not_null
          - relationships:
              to: ref('stg_customers')
              field: customer_id
```

Run `dbt docs generate` to generate the documentation for your project. a JSON file with rich documentation will be generated.

### Deploy dbt
Now that we've built our first model, all we have left to do is to commit the changes, by clicking the Commit and sync and adding a clear comment about the changes (e.g. Add customers model, tests, docs.), and then merge this branch to main.

We can then deploy the jobs to production sing dbt Cloud's scheduler. Select the Deploy tab on the upper left side, then click Environments.
Create a new environment called "Production" and the deployment type PROD.
For the dbt version select "Latest", set your connection to BigQuery and choose your BQ dataset, then click save on the top of the page.

Create a new job, selecting the Deploy job option.
On the Execution settings, the command should be  `dbt build` and check the Generate docs on run.

You can set-up triggers to run this job once another one is completed, or on a specific schedule, but for this tutorial there is no need to do that.

Save your Deploy job and click Run Now.

In Production, it's important to set up notifications in case one of the runs fails, you can do that in by Clicking your account name on the bottom left then Notification settings.

Congratulations ðŸŽ‰! You've just deployed your first dbt project!

### Conclusion

Now I know why dbt is not just a hype !

- First, it finally gives SQL analysts a Git workflow to do the job just like developers.
- Second, it makes tedious tasks, like testing and documenting, as simple as writing a couple of lines of code on a YAML file.
- Last but not least, it removes the need to write DDL statements, which in itself is a big value add in my opinion. No more "CREATE TABLE" or "ALTER VIEW" ever again !

So it makes sense that it has been heavily embraced, in such a short time. I can't wait to put a ton of data transformation heavy lifting on dbt.
