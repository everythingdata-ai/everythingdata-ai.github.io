---
layout: post
title: Microsoft Fabric - Analytics Engineer course (3/3)
categories: [Data Engineering, Azure]
---

Here is the third and last part of the DP600 exam preparation [course](https://www.youtube.com/watch?v=Bjk93hi21QM).
The part is mainly about opttimization/monitoring, semantic models and exploratory analytics.
Here are my notes from today.

## Chapter 8 : Design and build semantic models

Goals :
- Choose a storage mode, including Direct Lake
- Design and build composite models that include aggregations
- Design and build a large format dataset
- Implement a star schema for a semantic model
- Implement relationships, such as ridge tables
- Implement calculations
### Choose a storage mode

![Storage modes](/images/posts/fabric-storage.png)

With the import mode, you store a copy of your data in Power BI itself, so there are size limitations, and the data will have to be updated on regular basis.

On the other hand, with direct query, whenever the user views a report, he is sent back to the source and then getting fresh data in near-real time. The downsides are you can't transform the data in Power BI, and it can be slow for the user.

Direct Lake reads the parquet files of your data lake.

When to choose each one ?

![Access modes](/images/posts/fabric-access.png)


### Composite models

A composite model combines one or more of the connection modes above.
Commonly, this is a Direct Query for the Fact Table (since it's updated very often), and Import Mode for Dimension tables (since they are changing slower).

Composite models also provide a way to model many-to-many relationships, without the need for bridge tables.

### Aggregations

Aggregations are a specific feature within Power BI for aggregating large datasets and caching the result. They can be created in your data source, of directly in the Power Query engine using Import Mode.
The goal is to improve performance in large data models.
You can use 'Manage aggregations' dialog in PBI Desktop to define aggregations.
Automatic aggregations (Premium feature) use state-of-the-art ML to continuously optimize DirectQuery semantic models for maximum report query performance.

### Large format semantic models 

Normally used for over 100GB semantic models, they provide highly compressed in-memory cache for optimized query performance, enabling fast user interactivity.
Available on Premium P SKUs, Embedded A SKUs and with Premium Per User.
Used commonly when connecting third-party tools via the XMLA endpoint.
On-demand loading (same feature as Direct Lake)

### Implementing calculations in Power BI Desktop

- Variables : DAX variables can help avoid code repetition and improve performance.
You can create them with the keyword VAR VariableName = {Your expression} and the keyword RETURN.

```EXCEL
Average Revenue per Day = 
VAR TotalRevenue = SUM('revenue'[Revenue])
VAR TotalDays = DISTINCTCOUNT('date'[date])
RETURN
	IF(TotalDays > 0, TotalRevenue / TotalDays, BLANK())
```

- Iterators : enumerate through all rows in a table, performing some calculation and aggregating the result, like SUMX, COUNTX...

```EXCEL
Cumulative Revenue =
SUMX(
	FILTER(
		ALL('date'[date]),
		'date'[date] <= MAX('date'[date])
	),
	[Sum Revenue]
)
```


- Table filtering : the FILTER function returns a table that represents a subset of another table or expression

```EXCEL
Total Revenue by Category =
CALCULATE(
	SUM('revenue'[Revenue]),
	FILTER(
		'products',
		'products'[Product_Name] = SELECTEDVALUE('products'[Product_Name])
	)
)
```

- Window functions : allow you to perform calculations within a specific window of your table data. Some use cases might-be time-based (e.g. a 3 month moving average), or a window of a categorical variable (e.g. average revenue for each department in a company). There are 3 DAX function : WINDOW, INDEX and OFFSET

```EXCEL
3-day Average Price = 
AVERAGEX(
    WINDOW(
        -2,REL,0,REL,
        SUMMARIZE(ALLSELECTED('Sales'), 'Date'[Date], 'Product'[Product]),
        ORDERBY('Date'[Date]),
        KEEP,
        PARTITIONBY('Product'[Product])
    ), 
    CALCULATE(AVERAGE(Sales[Unit Price]))
)
```

- Information function : look at the cell or row that is provided and tells you whether the value matches the expected type. Useful examples : CONTAINS, CONTAINSSTRING, HASONEVALUE, ISBLANK, ISERROR, SELECTEDMEASURE, USERPRINCIPALNAME

```EXCEL
IsRevenueBlank =
IF(
	ISBLANK(SUM('SampleTransactions'[Revenue])),
	"No revenue recorded",
	"Revenue recorded"
)
```

- [Calculation groups](https://learn.microsoft.com/en-us/power-bi/transform-model/calculation-groups) : a simple way to reduce the number of measures in a model by grouping common measure expressions. 

- Dynamic string formatting : determine how measures appear in sivuals by conditionally applying a format string with a separate DAX expression. The difference with Format string of the modelling tab is that the data type is maintained.

```EXCEL
Dynamic Format Measure =
VAR SumRev = [Sum Revenue]
RETURN
	SWITCH(
		TRUE(),
		SumRev < 1000, FORMAT(SumRev, "0.00"),
		SumRev < 1000000, FORMAT(SumRev/1000, "O.OK"),
		SumRev < 1000000000, FORMAT(SumRev/1000000, "O.OM"),
		FORMAT(SumRev/1000000000, "O.OB")
	)
```

- Field parameters : allow the report user to select a categoric variable to view a particular measure by. (e.g. revenue by location ,dealer, country, model...)
You can set it up in the Modeling tab > New parameter

## Chapter 9 : Optimize entreprise-scale semantic models

Goals :
- Implement dynamic row-level security and object-level security
- Implement incremental refresh
- Implement performance improvements in queries and report visuals
- Identify use cases for DAX Studio and Tabular Editor 
- Improve DAX performance using DAX Studio

### Dynamic row-level security (in semantic models)

In general, row-level security restricts who can see what data (at row-level) in specific tables in your Power BI report.
Dynamic RLS of a method of applying RLS using the UserPrincipalName() information function that gives the e-mail address of a user.

You can configure RLS in the semantic model, the data warehouse and the T-SQL endpoint of the lakehouse. If you're using Direct Lake mode, RLS must be configured in the semantic model.

RLS only restricts data access for users with Viewer permissions, it doesn't affect Admins, Members or Contributors.

High-level steps for RLS implementation :
1) Create a role
2) Select the table you want to apply RLS to
3) Enter a table filter DAX expression to configure when/who the RLS is applied
4) Validate the RLS has been applied correctly

### Object-level security (Tabular Editor)

With OLS, you can configure who can view tables and specific columns within a PBI report.
To configure it, you need an external tool such as Tabular Editor.
Similarly to RLS, only applies to viewers.

High-level steps for OLS implementation :
1) Create a Role (in PBI Desktop)
2) Open Tabular Editor, find the Role, click on Table Properties for that Role
3) Set the permissions for the table to None or Read
4) Publish the report to the Service, and add people/groups to the Role

### Incremental refresh

Typically used on large Fact tables, allows you to pull in only the data that has changed in a given time period (e.g. last 24 hours), rather than a full load from the source.

Benefits : 
- Fewer refreshes are required
- Refreshes are a lot quicker
- Resource consumption is reduced
- Refreshes can be more reliable

Available only for PBI Premium.

Implementation :
- Create RangeStart and RangeEnd parameters
- In Power Query, apply custom date filter using the RangeStart and RangeEnd 
- Define your incremental refresh policy

### Monitoring semantic model performance

For analyzing Power Query performance, you use the Query analyzer tool

For analyzing visual and query performance, you can use Performance Analyzer in PBI Desktop (Optimize tab). You can load this data into DAX Studio for further analysis

For DAX performance, you can use DAX Studio. You can use View Metrics (Advanced Tab) to look at the VertiPaq Analyzer : review table and column sizes (cardinality, data types...), look for referential integrity violations (mismatch in unique keys on two sides of a relationship)
You can also do Trace analysis.

For semantic model performance, you can use the Best Practice Analyzer in Tabular Editor (Tools menu). It performs a scan of your semantic model and checks for common issues.
The list of rules can be downloaded from GitHub, and are organized into the following categories :
- Performance
- DAX Expressions
- Error Prevention
- Formatting
- Maintenance

### Use cases for DAX Studio and Tabular Editor

DAX Studio use cases
- Write, execute, and debug DAX queries (need to be manually copied over to Power BI Desktop, as DAX Studio is 'Read-Only'.
- Use the VertiPaq Analyzer to understand the size of your semantic model (as well as individual tables and columns, within the model).
- Bring Power BI Desktop Performance Analyzer data into DAX Studio for further analysis.
- Perform trace analysis

Tabular Editor use cases
- Quickly edit data models - create measures, perspectives, calculation groups from the DAX editor (and publish them into a semantic model)
- Automate repetitive tasks using scripting
- Incorporate DevOps with tabular models
- Use the Best Practice Analyzer to identify common issues in Power BI semantic models.

## Chapter 10 : Perform exploratory analytics

Goals :
- Implement descriptive and diagnostic analytics
- Integrate prescriptive and predictive analytics into a visual report
- Profile data

### Types of analytics :

- Descriptive : interpret past data and KPIs to identify trends and patterns (**describe > what happened**)
- Diagnostic : by focusing on past performance data, decide which data element will influence specific trends and the possibility of any future events, using techniques like data mining and correlation (**diagnose > why it happened**)
- Predictive : using statistics to forecast future outcomes, to provide context and clarity for future decisions (**predict > what will happen**)
- Prescriptive : build on descriptive and predictive analytics to recommend specific actions that ensure the best or most profitable customer reactions and business outcomes possible (**prescribe > a course of action to take**)

### Power BI visuals (and when to choose them)

- Table and Matrix visualizations : good for fine grained details and drill-throughs. Can be used to display aggregate information (e.g. monthly revenue). It's difficult to spot long-term trends.

- Bar and column charts : when you have one categoric variable and one numeric variable (revenue by region). Can be stacked to show more than one categoric variable. Not well suited to time-series.

- Line and area charts : well-suited for time-series. Legend can be used to show how a metric changes within each category. Area chart can be difficult to interpret.

- Card visualizations : good for KPI metrics, can also include % change metrics, which give context. Don't give longer-term trends (can be coupled with a spark line to show momentum)

- Pie chart, donut chart, tree maps : can be used to show rations. Difficult for human brain to compare between categories. Can't see trends over time. Obscures overall numbers.

- Combo charts : used to visualize more than one metric (line chart on top of bar chart for example). Can be difficult for user to interpret.

- Funel visualizations : can show movement through a linear process (e.g. tracking website conversion). Difficult for user to grasp the scale of difference.

- Gauge chart : can show progress of a particular metric towards a goal.

- Waterfall visualizations : show a running total over a time period (or categories.) Can show which periods or categories contributed to change the most. Can be difficult to interpret.

- Scatter charts : visualising two numeric variables (and therefore implied correlation). Dots can be color-coded to deepen the analysis. Can sometimes lead to correlation conclusions (which != causation)

- Custom visuals (appsource) : can go beyond the out-the-box solution. However involve some licensing/paywall considerations.

- Q&A visualization : allow the user to ask natural language questions about your data

### Extra Power BI features

- Drilldown : explore data through layers of a hierarchy (Revenue by country > state > city > store)

- Drillthrough : drill-through to a separate report page, to show more detailed information for that given category.

- Grouping : group two or more categories 

- Binning : create bins for continuous variables (e.g. from continuous salary variable to salary ranges - 0 to 30k / 30k to 60k / 60k to 90k)

- Reference lines : provide a static reference line across an axis to give context (e.g. average sales)

- Visualising errors : important tool to visualise uncertainty

- What-if parameters : rudimentary scenario analysis

- Time-series forecasting : add a forecast line to time-series/continuous variables (with error bounds)

### Power Query data profiling tool

In Power Query go to the View tab > Data View > Enable column profile.

Column profiling is based on the first 1000 rows, you can change that on the bottom to consider the entire data set.

That's the end of this course ! 
I think this video is a great introductio to the Microsoft Fabric universe.
I will continue exploring more courses in preparation of the DP600 or DP700 certification.


