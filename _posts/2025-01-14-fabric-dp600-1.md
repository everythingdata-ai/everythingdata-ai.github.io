---
layout: post
title: Microsoft Fabric - Analytics Engineer course (1/3)
categories: [Data Engineering, Azure]
---

These days I decided to continue digging deeper in the Microsoft Fabric environment.
I found an interesting [course](https://www.youtube.com/watch?v=Bjk93hi21QM) intended as a preparation for the Analytics Engineer certification.
Here are my notes about the first section of the exam study guide : Plan, implement and manage a solution for data analytics

## Chapter 1 : Plan a data analytics environment

Goals : 
- Identify requirements for a solution, including components, features, performance, and capacity SKUs
- Recommend settings in the Fabric admin portal
- Choose a data gateway type
- Create a custom Power BI report theme

### Requirements gathering workshop :
You organize a workshop with a client to understand their business needs and requirements.
Your goal is to build a plan for their new Fabric environment and also get a contract to do the building.

What are you going to ask ?

#### 1) Capacities : the number of capacities required and their sizes

What determines the number of capacities ?

- Compliance with data residency regulations. If you data must be in the EU (GDPR regulations) that's one capacity. If you have other requirements for some datasets that should be stored in the US, that's an extra capacity.

- The billing preference within the organisation ,some companies prefer to separate their billing by department, so you will have one capacity for each one.

- Segregating by workload type (i.eI DE & BI). If you have a lot of heavy DE workloads, put them in a separate capacity. Serving BI dashboards can be done in a separate capacity to not be impacted by the DE capacity.

- Segregating by department, some companies want their capacity linked to a department.

After we determined the number of capacities, we need to determine their sizing.

- Intensity of expected workload : high volumes of data ingestion (Terrabytes every day) or heavy transformations (Spark) or machine learning training will use a lot of resources and require a high capacity

- The budget of the client dictates the sizing, the more resources the more expensive the bill will be and some clients and not prepared to put in the price.

- Can you afford to wait ? For some business, the tasks taking a lot of time is not an issue, making you can do the calculations overnight. 

- Access to F64+ features : there are some features that are only available when you get to F64 like Copilot
 
#### 2) Data ingestion methods : 

The requirements we needs are :
- the Fabric items/features you will need to get data into Fabric
- how these items/features will need to be configured

Some of the available ingestion options are :
Shortcut, Database mirroring, ETL (Dataflow, Data pipeline, Notebooks), Eventstream
Some other available feature : On-premise data gateway, VNet data gateway, Fast copy, Staging

Deciding factors : 
Where is the external data stored ?
- ADLS Gen2, Amazon S3 (or S3 backed), Google Cloud Storage or Dataverse ? You can Shortcut into these
- Azure SQL, Azure Cosmos DB, Snowflake ? Database mirroring
- On-premise SQL ? ETL using Dataflow or Data pipeline
- Real-time events ? Eventsream
- Other ? One of the 3 ETLs

What skills exist in the team ?
- Predominantly no/low-code ? Dataflow and Data pipeline
- SQL ? Data pipeline
- Spark (Python Scala...) ? Notebook

How is the data secured ?
- On-premise SQL ? On-premise data gateway
- Azure virtual network / private endpoint ? VNet data gatewat

What is the volume of the data ?
- Low (megabytes per day) - out of the box options
- Medium (gigabytes per day) - Fast copy / Staging
- High (many GB or terabytes per day) - Fast copy / Staging / ETL


Before we move on, let's explore data gateways in more details.
Data gateways allows us to access (in a secure way) data that is otherwise secured.

There are 2 types that we can configure on MS Fabric. Here are the steps to do that for each one : 
On-premise data gateway :
- Install the data gateway on the on-premise server 
- In Fabric, create a new on-premise data gateway connection
- Use the gateway in a Dataflow or Data pipeline to get data into Fabric

Virtual network (VNet) data gateway :
- Set network configuration in Azure (Register Power Platform resource provider > Create a private endpoint on your Azure object > Create a subnet)
- In Fabric, create a new virtual network Data Gateway connection
- Use the gateway in a Dataflow or Data Pipeline to get data into Fabric


#### 3) Data storage options :

The requirements we need are the Fabric data stores and the overall architectural pattern (medallion, lambda...)

Some storage options in Fabric are : Lakehouse, Data warehouse, KQL database.

Deciding factors :

What is the data type ?
- Structured, semi-structured and/or unstructured ? Lakehouse
- Relational/structured ? Either lakehouse or warehouse
- Real-time/streaming ? KQL

What skills exist in the team ?
- T-SQL ? warehouse
- Spark ? lakehouse
- KQL ? KQL

### Fabric admin workshop

After completing your first engagement for the client and helped convince them set-up a Proof-of-Concept project, you got a phone call requesting help navigating the Fabric admin portal.

How are you going to teach them ?

The admin portal can be accessed by admins with the following roles : 
- Global administrator
- Power platform administrator
- Fabric administrator

#### Tenant settings : 
Allow users to create Fabric items, enable preview features, allow users to create workspaces, security features (guest users, sign-on for snowflake/bigquery/redshift, block public internet access, enable azure private link...), allow service principal access to Fabric APIs, allow Git integration, allow copilot

Some setting can be :
- Enabled for the entire org
- Enable for specific security gouprs
- Enable for all EXCEPT certain security groupes 

#### Capacity settings :
Allows users to create/delete new capacities, manage capacity permissions and change the size of the capacity

### Power BI them workshop

The client asks for one last thing, they want the BI team to create more consistent reports, how to achieve that ?

You can create custom report theme for power BI by :
- updating the current theme from Power BI Desktop
- writing a new JSON template yourself
- Using a third-party online tool to generate a theme for you


## Chapter 2 : Implement and manage a data analytics environment

Goals :
- Implement workspace and item-level access controls for Fabric items
- Implement data sharing for workspaces, warehouses and lakehouses
- Manage sensitivity labels in semantic models and lakehosues
- Configure Fabric-enabled workspace settings

You have won the contract to implement the new solution after successfully planning the data environment of the client in Chapter 1. Your client wants you to help structure the Fabric implementation. How can you help ?

This is the high level structure, which is hierarchical, from top to bottom :

Tenant-level : one tenant
Capacity-level : one or many capacities
Workspace-level : one or many workspaces per capacity
Item-level : datawarehouse, lakehouse, semantic model...
Object-level : Dbo.Customer, VW.MySQLView...

### Capacity administrator settings 

Capacity administrations tasks in Azure : 
- Create a new capacity
- Delete a capacity
- Change the size
- Change the capacity administrator
- Pausing/resuming a capacity

Capacity administrations tasks in Fabric : 
- Enable disaster recovery
- Capacity usage report
- Define who can create workspaces
- Update Power BI connection settings from/to this capacity
- Permiet workspace admins to size their own custom Spark pools

### Workspace administrator settings 

A workspace administrator can :
- Edit license for the workspace (pro, ppu, fabric, trial...)
- Configure Azure connections
- Configure Azure DevOps connection (Git)
- Setup workspace identity
- Power BI settings
- Spark settings
### Workspace-level sharing

People or groups can be given workspace-level access, with one of these roles :
- Admin
- Member 
- Contributor
- Viewer
### Sensitivity label

Sensitivity labels are a data governance feature.
Labels are created and managed in Microsoft Purview.
Fabric items such as a semantic model, can be given a sensitivity label such as 'Confidential'. This is for information protection.
In some industries, a sensitivity label is necessary for compliance with information protection regulations.

## Chapter 3 : Managing the analytics development lifecycle

Goals : 
- Implement version control
- Create and manage a Power BI Desktop project
- Plan and implement deployment solutions
- Perform impact analysis of downstream dependencies 
- Deploy and manage semantic models
- Create and update reusable assets

### Version control

Version control allows you to track changes, revert to older versions, and collaboration between multiple users. It also implements a check and approval process for changes. 

The basics of Git/version control :
- Create an Azure DevOps project or Github repository
- Generate a Github token
- Go to Workspace settings > Git integration and connect your repository

It's better to protect the main branch by enabling the "Require a pull request before merging" option in Github.

### Deployment solutions

Rather than having one copy of a Power BI report (the production copy), we typically have three :
- a development version : the version you use when you are making changes to a live report
- a test version : you might give to colleagues/a client to test/review
- a production report : public and shared with stakeholders

Microsoft released a feature called deployment pipelines to help manage these environments.
To use it click on Workspaces > Deployment pipelines

Each environment should have a separate workspace.
Deployment rules can be implemented to change things like the Default Lakehouse (for a notebook) for different stages.

For semantic models, you can deploy them using XMLA endpoints.

### Power BI file types

- Power BI template file (.pbit) : a reusable asset which can improve efficiency and consistency when creating reports. You can easily save a pbix file as a pbit file, then generate a new report from that template. You will be asked to input parameter values if there are any. 

- Power BI data source file (.pbids) : helps us quickly transfer all the data connections from one report into another.
