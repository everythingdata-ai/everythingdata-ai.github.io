---
layout: post
title: Astronomer Airflow Academy
categories: [Data Engineering, Airflow]
---

These are my notes from the Airflow Academy by Astronomer.
You can check out the free course [here](https://academy.astronomer.io/path/airflow-101)
This course is a good preparatoin for the Apache Airflow fundamentals certification.
This certification covers the basics : your understanding of Airflowâ€™s architecture, core components, and how to use it effectively for building and monitoring workflows.

### Introduction to Orchestration and Airflow

#### Why orchestration ?
Let's say you're a Data Engineer who wants to ingest data from databases and 3rd party sources & APIs into a data warehouse.
Then you have a data analyst who wants to access the data warehouse to transfor the data through dbt and make dashboards.
Also there a data scientist team that are building models on top of the data warehouse, and machine learning team industrializing those models.

With data orchestration, you are able to build data pipelines that coordinate and automate data flow across various tools and systems.

#### What is Airflow ?
Airflow is a modern data open-source orchestration tool, for programatically authoring, scheduling and monitoring your data pipelines.

Airflow is defined by :
- the rise of pipelines-as-code in Python
- the ability to intergrate with hundreds of external systems
- time and event-based scheduling
- feature-rich software

Airflow has become the de-facto choise for orchestration.

#### Who uses Airflow ?
- Data scientists : pre-process and analyze new datasets
- Data engineers : creating new datasets and managing data pipelines
- ML engineers : retraining models automatically
- Data Analyst : automatically refresh reports after every incremental load

#### Use cases
There are 4 main use cases of Airflow :
- Data-powered applications  
- Critical operational processes
- Analytics & reporting
- MLOps & AI

Aiflow is designed for Batch processing, not streaming processing.
However, you can still mix Airflow with Kafka.

By default, Airflow is scalable.
Consider the resources and infrastructure required as the complexity increases.

Airflow can process data, but it's designed mainly for orchestrating and managing data workflows.
Make sure you have enough resources if you're using it for process.

#### How does Airflow work ?

The first key-term is DAG : Directed Acyclic Graph, a.k.a a single data pipeline
Directed : DAG has an order
Acyclic : DAG has no cycles

The second key-term is Task : a single unit of work in a DAG

Third key-term to remember is Operator : defines what the task does (for example execute a Python function)
there are 3 main categories ofoperators :
- Action operators : execute python, execute a sql query...
- Transfer operators : transfer data from S3 to Snowflake...
- Sensor operators : wait for an event like uploading a file

### Basics

#### Airflow core components
Airflow has 6 main components : 
- Web server : enable user to monitor task execution
- Metadata database : a db containing task instances, DAG status, users...
- Scheduler : determins which task to run and when
- Executor : part of the scheduler, determins how and on which system to execute your tasks
- Queu : part of the executor, manages the order of execution
- Worker : takes the tasks out of the queue to execute them

#### How does Airflow run a DAG ?
1. The Airflow constantly scans the DAGs directory for new files (every 5 minutes)
2. The Airflow scheduler serializes the new DAG and stores it in the metadata database
3. The Airflow scheduler scans the metadata database to see if any DAGs are ready to run
4. The scheduler sends the DAG's tasks into the executors queue when it's ready to run
5. The task is executed when a worker is available.

#### Key components of a DAG file
A DAG file can be separated into 4 parts :

- Import statements : import the dag library to let airflow know that this is a DAG

```python
from airflow import dag
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
```

- DAG object : define the data pipeline and its parameters (when to tigger it for example)

```python
with DAG(
  ...
):
```

- Tasks : where you define the work that will be done

```python
  first_task = PythonOperator(
    ...
  )
  second_task = BashOperator(
    ...
  )
```

- Dependencies : relationship between tasks and execution order

```python
first_task >> second_task
```

### Local Development Environment

N/A, I already have an Airflow up and running on GCP.

Quiz : 

- Q: What is the purpose of the include directory in a new Airflow project generated by the Astro CLI?
- A: For storing files like SQL queries, bash scripts, or Python functions needed in data pipelines to keep them clean and organized

- Q: What is the purpose of the airflow_settings.yaml file in a new Airflow project generated by the Astro CLI?
- A: For storing configurations such as connections and variables to prevent loss when recreating the local development environment
   
### UI

One of the main features of Airflow is its user interface (UI), which provides insights into your DAGs and DAG runs. The UI is essential for understanding, monitoring, and troubleshooting your pipelines.

This module overviews some of the Airflow UI's most valuable features and visualizations. 

#### Lnading page

The landing page of Airflow is the DAG page.
For each DAG, you have from left to right :
- Toggle to pause/unpause your DAG
- DAG name (unique identifier) with a tag under it to organize your DAGS
- Owner
- Runs, statues of your current runs
- Schedule, shows how often your pipeline is trigerred
- Last run
- Next run
- Recent tasks

<img width="737" alt="image" src="https://github.com/user-attachments/assets/455844c7-db85-4c06-8206-65f49a946c27" />

On the right, you have the actions buttons as well as the Links to see other views :
- Trigger the DAG manually
- Delete the DAG (not the file, only the metadata)

<img width="53" alt="image" src="https://github.com/user-attachments/assets/9f2f8c42-3bf6-4382-bb40-cde53289ea25" />

#### Grid view and Graph view
If you click on a DAG, you land in the Grid view that gives insights of the DAG : 

![image](https://github.com/user-attachments/assets/e74f9a2e-264d-4cbf-90a3-1605da91dbeb)

And from there you can go to the Graph view, which is perfect for checking dependencies between tasks of a DAG :

![image](https://github.com/user-attachments/assets/2bad36e5-90d3-4170-9488-dc6d0eca2067)

Tip : The mean run duration + buffer is the correct value for defining a timeout for your DAG.

#### Calendar view and Landing times

Another useful view is the Calendar view, it overviews your entire DAG's history over months or even years, making it easy to spot breaking patterns or trends.
A square is a day, and the color of that square depends on the ratio between successful and failed DAG runs.
Squares with dots indicate that DAG runs are planned for these days.

Landing times are calculated from the task scheduled time to the time the task finishes (end_time - scheduled_time)
This view is perfect for evaluating the effectiveness of your changes.

#### Gantt view

This view helps you find bottlenecks in your DAG runs.
A rectangle is a task. The longer the rectangle, the longer it took to complete the task.
A rectangle is divided into two parts. The first part (grey) is the queued time. The second is the execution time.
The queued time corresponds to the time spent waiting for a worker to pick your task. A worker executes tasks.
Two rectangles side by side mean tasks have been executed in parallel and this is an overlap.

#### Code view 

The Code view shows the DAG code parsed by the Scheduler.
The Code view is perfect for checking whether or not DAG updates have been picked up.
Look at the Parsed at date instead of searching for your modification in the code.
You can't edit DAG code in the Code view.

### DAGs 101

In this module we will learn the basics of how to create a data pipeline in Airflow.

#### What's a DAG?

In Airflow, a directed acyclic graph (DAG) is a data pipeline defined in Python code. Each DAG represents a collection of tasks you want to run and is organized to show relationships between tasks in the Airflow UI. The mathematical properties of DAGs make them useful for building data pipelines:

Directed: If multiple tasks exist, each must have at least one defined upstream or downstream task.
Acyclic: Tasks cannot have a dependency on themselves. This avoids infinite loops.
Graph: All tasks can be visualized in a graph structure, with relationships between tasks defined by nodes and vertices.
Aside from these requirements, DAGs in Airflow can be defined however you need! They can have a single task or thousands of tasks arranged in any number of ways.

For example, the image below represents a DAG with three tasks.

![image](https://github.com/user-attachments/assets/8ae9bd88-a972-4acb-8b3f-1c18b8e0d957)

#### Your first data pipeline

Let's create a new file in the dag folder called my_dag.py.

Airflow offers two ways of defining your DAGs and Tasks:

- The traditional paradigm :


```python
from airflow import DAG
from datetime import datetime

with DAG('my_dag',
  description='A simple description',
  tags=[data_science],
  start_date=datetime(2025, 1, 1),
  schedule='@daily',
  catchup=False):
None
```
- The TaskFlow API :

```python
from airflow.decorators import dag
from datetime import datetime

@dag(start_date=datetime(2023, 1, 1), description='A simple tutorial DAG', 
     tags=['data_science'], schedule='@daily', catchup=False)
def my_dag():
    None

my_dag()
```

With the Taskflow API, instead of using the context manager with, you use the dag decorator.
The dag decorator expects a python function (here my_dag) where the name corresponds to your DAG id.
You will define/call tasks within this function.
Last but not least, do not forget to call your dag function ( here my_dag()) otherwise your DAG won't show up on the Airflow UI.


### DAG scheduling

### Connections 101

### XComs 101

### Variables 101

### Debug DAGs

### Sensors

### Command Line Interface

### 
