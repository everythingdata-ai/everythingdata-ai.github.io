---
layout: post
title: Astronomer Airflow Academy
categories: [Data Engineering, Airflow]
---

These are my notes from the Airflow Academy by Astronomer.
You can check out the free course [here](https://academy.astronomer.io/path/airflow-101)
This course is a good preparatoin for the Apache Airflow fundamentals certification.
This certification covers the basics : your understanding of Airflowâ€™s architecture, core components, and how to use it effectively for building and monitoring workflows.

### Introduction to Orchestration and Airflow

#### Why orchestration ?
Let's say you're a Data Engineer who wants to ingest data from databases and 3rd party sources & APIs into a data warehouse.
Then you have a data analyst who wants to access the data warehouse to transfor the data through dbt and make dashboards.
Also there a data scientist team that are building models on top of the data warehouse, and machine learning team industrializing those models.

With data orchestration, you are able to build data pipelines that coordinate and automate data flow across various tools and systems.

#### What is Airflow ?
Airflow is a modern data open-source orchestration tool, for programatically authoring, scheduling and monitoring your data pipelines.

Airflow is defined by :
- the rise of pipelines-as-code in Python
- the ability to intergrate with hundreds of external systems
- time and event-based scheduling
- feature-rich software

Airflow has become the de-facto choise for orchestration.

#### Who uses Airflow ?
- Data scientists : pre-process and analyze new datasets
- Data engineers : creating new datasets and managing data pipelines
- ML engineers : retraining models automatically
- Data Analyst : automatically refresh reports after every incremental load

#### Use cases
There are 4 main use cases of Airflow :
- Data-powered applications  
- Critical operational processes
- Analytics & reporting
- MLOps & AI

Aiflow is designed for Batch processing, not streaming processing.
However, you can still mix Airflow with Kafka.

By default, Airflow is scalable.
Consider the resources and infrastructure required as the complexity increases.

Airflow can process data, but it's designed mainly for orchestrating and managing data workflows.
Make sure you have enough resources if you're using it for process.

#### How does Airflow work ?

The first key-term is DAG : Directed Acyclic Graph, a.k.a a single data pipeline
Directed : DAG has an order
Acyclic : DAG has no cycles

The second key-term is Task : a single unit of work in a DAG

Third key-term to remember is Operator : defines what the task does (for example execute a Python function)
there are 3 main categories ofoperators :
- Action operators : execute python, execute a sql query...
- Transfer operators : transfer data from S3 to Snowflake...
- Sensor operators : wait for an event like uploading a file

### Basics

#### Airflow core components
Airflow has 6 main components : 
- Web server : enable user to monitor task execution
- Metadata database : a db containing task instances, DAG status, users...
- Scheduler : determins which task to run and when
- Executor : part of the scheduler, determins how and on which system to execute your tasks
- Queu : part of the executor, manages the order of execution
- Worker : takes the tasks out of the queue to execute them

#### How does Airflow run a DAG ?
1. The Airflow constantly scans the DAGs directory for new files (every 5 minutes)
2. The Airflow scheduler serializes the new DAG and stores it in the metadata database
3. The Airflow scheduler scans the metadata database to see if any DAGs are ready to run
4. The scheduler sends the DAG's tasks into the executors queue when it's ready to run
5. The task is executed when a worker is available.

#### Key components of a DAG file
A DAG file can be separated into 4 parts :

- Import statements : import the dag library to let airflow know that this is a DAG

```python
from airflow import dag
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
```

- DAG object : define the data pipeline and its parameters (when to tigger it for example)

```python
with DAG(
  ...
):
```

- Tasks : where you define the work that will be done

```python
  first_task = PythonOperator(
    ...
  )
  second_task = BashOperator(
    ...
  )
```

- Dependencies : relationship between tasks and execution order

```python
first_task >> second_task
```

### Local Development Environment

N/A, I already have an Airflow up and running on GCP.

Quiz : 

- Q: What is the purpose of the include directory in a new Airflow project generated by the Astro CLI?
- A: For storing files like SQL queries, bash scripts, or Python functions needed in data pipelines to keep them clean and organized

- Q: What is the purpose of the airflow_settings.yaml file in a new Airflow project generated by the Astro CLI?
- A: For storing configurations such as connections and variables to prevent loss when recreating the local development environment
   
### UI

One of the main features of Airflow is its user interface (UI), which provides insights into your DAGs and DAG runs. The UI is essential for understanding, monitoring, and troubleshooting your pipelines.

This module overviews some of the Airflow UI's most valuable features and visualizations. 

#### Lnading page

The landing page of Airflow is the DAG page.
For each DAG, you have from left to right :
- Toggle to pause/unpause your DAG
- DAG name (unique identifier) with a tag under it to organize your DAGS
- Owner
- Runs, statues of your current runs
- Schedule, shows how often your pipeline is trigerred
- Last run
- Next run
- Recent tasks

<img width="737" alt="image" src="https://github.com/user-attachments/assets/455844c7-db85-4c06-8206-65f49a946c27" />

On the right, you have the actions buttons as well as the Links to see other views :
- Trigger the DAG manually
- Delete the DAG (not the file, only the metadata)

<img width="53" alt="image" src="https://github.com/user-attachments/assets/9f2f8c42-3bf6-4382-bb40-cde53289ea25" />

#### Grid View and Graph view
If you click on a DAG, you land in the Grid view that gives insights of the DAG : 

![image](https://github.com/user-attachments/assets/e74f9a2e-264d-4cbf-90a3-1605da91dbeb)

And from there you can go to the Graph view, which is perfect for checking dependencies between tasks of a DAG :

![image](https://github.com/user-attachments/assets/2bad36e5-90d3-4170-9488-dc6d0eca2067)


Tip : The mean run duration + buffer is the correct value for defining a timeout for your DAG.

#### 

### DAGs 101

### DAG scheduling

### Connections 101

### XComs 101

### Variables 101

### Debug DAGs

### Sensors

### Command Line Interface

### 
