---
layout: post
title: Astronomer Airflow Academy
categories: [Data Engineering, Airflow]
---

These are my notes from the Airflow Academy by Astronomer.
You can check out the free course [here](https://academy.astronomer.io/path/airflow-101)
This course is a good preparatoin for the Apache Airflow fundamentals certification.
This certification covers the basics : your understanding of Airflow’s architecture, core components, and how to use it effectively for building and monitoring workflows.

### Introduction to Orchestration and Airflow

#### Why orchestration ?
Let's say you're a Data Engineer who wants to ingest data from databases and 3rd party sources & APIs into a data warehouse.
Then you have a data analyst who wants to access the data warehouse to transfor the data through dbt and make dashboards.
Also there a data scientist team that are building models on top of the data warehouse, and machine learning team industrializing those models.

With data orchestration, you are able to build data pipelines that coordinate and automate data flow across various tools and systems.

#### What is Airflow ?
Airflow is a modern data open-source orchestration tool, for programatically authoring, scheduling and monitoring your data pipelines.

Airflow is defined by :
- the rise of pipelines-as-code in Python
- the ability to intergrate with hundreds of external systems
- time and event-based scheduling
- feature-rich software

Airflow has become the de-facto choise for orchestration.

#### Who uses Airflow ?
- Data scientists : pre-process and analyze new datasets
- Data engineers : creating new datasets and managing data pipelines
- ML engineers : retraining models automatically
- Data Analyst : automatically refresh reports after every incremental load

#### Use cases
There are 4 main use cases of Airflow :
- Data-powered applications  
- Critical operational processes
- Analytics & reporting
- MLOps & AI

Aiflow is designed for Batch processing, not streaming processing.
However, you can still mix Airflow with Kafka.

By default, Airflow is scalable.
Consider the resources and infrastructure required as the complexity increases.

Airflow can process data, but it's designed mainly for orchestrating and managing data workflows.
Make sure you have enough resources if you're using it for process.

#### How does Airflow work ?

The first key-term is DAG : Directed Acyclic Graph, a.k.a a single data pipeline
Directed : DAG has an order
Acyclic : DAG has no cycles

The second key-term is Task : a single unit of work in a DAG

Third key-term to remember is Operator : defines what the task does (for example execute a Python function)
there are 3 main categories ofoperators :
- Action operators : execute python, execute a sql query...
- Transfer operators : transfer data from S3 to Snowflake...
- Sensor operators : wait for an event like uploading a file

### Basics

#### Airflow core components
Airflow has 6 main components : 
- Web server : enable user to monitor task execution
- Metadata database : a db containing task instances, DAG status, users...
- Scheduler : determins which task to run and when
- Executor : part of the scheduler, determins how and on which system to execute your tasks
- Queu : part of the executor, manages the order of execution
- Worker : takes the tasks out of the queue to execute them

#### How does Airflow run a DAG ?
1. The Airflow constantly scans the DAGs directory for new files (every 5 minutes)
2. The Airflow scheduler serializes the new DAG and stores it in the metadata database
3. The Airflow scheduler scans the metadata database to see if any DAGs are ready to run
4. The scheduler sends the DAG's tasks into the executors queue when it's ready to run
5. The task is executed when a worker is available.

#### Key components of a DAG file
A DAG file can be separated into 4 parts :

- Import statements : import the dag library to let airflow know that this is a DAG

```python
from airflow import dag
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
```

- DAG object : define the data pipeline and its parameters (when to tigger it for example)

```python
with DAG(
  ...
):
```

- Tasks : where you define the work that will be done

```python
  first_task = PythonOperator(
    ...
  )
  second_task = BashOperator(
    ...
  )
```

- Dependencies : relationship between tasks and execution order

```python
first_task >> second_task
```

### Local Development Environment

N/A, I already have an Airflow up and running on GCP.

Quiz : 

- Q: What is the purpose of the include directory in a new Airflow project generated by the Astro CLI?
- A: For storing files like SQL queries, bash scripts, or Python functions needed in data pipelines to keep them clean and organized

- Q: What is the purpose of the airflow_settings.yaml file in a new Airflow project generated by the Astro CLI?
- A: For storing configurations such as connections and variables to prevent loss when recreating the local development environment
   
### UI

One of the main features of Airflow is its user interface (UI), which provides insights into your DAGs and DAG runs. The UI is essential for understanding, monitoring, and troubleshooting your pipelines.

This module overviews some of the Airflow UI's most valuable features and visualizations. 

#### Lnading page

The landing page of Airflow is the DAG page.
For each DAG, you have from left to right :
- Toggle to pause/unpause your DAG
- DAG name (unique identifier) with a tag under it to organize your DAGS
- Owner
- Runs, statues of your current runs
- Schedule, shows how often your pipeline is trigerred
- Last run
- Next run
- Recent tasks

<img width="737" alt="image" src="https://github.com/user-attachments/assets/455844c7-db85-4c06-8206-65f49a946c27" />

On the right, you have the actions buttons as well as the Links to see other views :
- Trigger the DAG manually
- Delete the DAG (not the file, only the metadata)

<img width="53" alt="image" src="https://github.com/user-attachments/assets/9f2f8c42-3bf6-4382-bb40-cde53289ea25" />

#### Grid view and Graph view
If you click on a DAG, you land in the Grid view that gives insights of the DAG : 

![image](https://github.com/user-attachments/assets/e74f9a2e-264d-4cbf-90a3-1605da91dbeb)

And from there you can go to the Graph view, which is perfect for checking dependencies between tasks of a DAG :

![image](https://github.com/user-attachments/assets/2bad36e5-90d3-4170-9488-dc6d0eca2067)

Tip : The mean run duration + buffer is the correct value for defining a timeout for your DAG.

#### Calendar view and Landing times

Another useful view is the Calendar view, it overviews your entire DAG's history over months or even years, making it easy to spot breaking patterns or trends.
A square is a day, and the color of that square depends on the ratio between successful and failed DAG runs.
Squares with dots indicate that DAG runs are planned for these days.

Landing times are calculated from the task scheduled time to the time the task finishes (end_time - scheduled_time)
This view is perfect for evaluating the effectiveness of your changes.

#### Gantt view

This view helps you find bottlenecks in your DAG runs.
A rectangle is a task. The longer the rectangle, the longer it took to complete the task.
A rectangle is divided into two parts. The first part (grey) is the queued time. The second is the execution time.
The queued time corresponds to the time spent waiting for a worker to pick your task. A worker executes tasks.
Two rectangles side by side mean tasks have been executed in parallel and this is an overlap.

#### Code view 

The Code view shows the DAG code parsed by the Scheduler.
The Code view is perfect for checking whether or not DAG updates have been picked up.
Look at the Parsed at date instead of searching for your modification in the code.
You can't edit DAG code in the Code view.

### DAGs 101

In this module we will learn the basics of how to create a data pipeline in Airflow.

#### What's a DAG?

In Airflow, a directed acyclic graph (DAG) is a data pipeline defined in Python code. Each DAG represents a collection of tasks you want to run and is organized to show relationships between tasks in the Airflow UI. The mathematical properties of DAGs make them useful for building data pipelines:

Directed: If multiple tasks exist, each must have at least one defined upstream or downstream task.
Acyclic: Tasks cannot have a dependency on themselves. This avoids infinite loops.
Graph: All tasks can be visualized in a graph structure, with relationships between tasks defined by nodes and vertices.
Aside from these requirements, DAGs in Airflow can be defined however you need! They can have a single task or thousands of tasks arranged in any number of ways.

For example, the image below represents a DAG with three tasks.

![image](https://github.com/user-attachments/assets/8ae9bd88-a972-4acb-8b3f-1c18b8e0d957)

#### Your first data pipeline

Let's create a new file in the dag folder called my_dag.py.

Airflow offers two ways of defining your DAGs and Tasks:

- The traditional paradigm :

```python
from airflow import DAG
from datetime import datetime

with DAG('my_dag',
  description='A simple description',
  tags=[data_science],
  start_date=datetime(2025, 1, 1),
  schedule='@daily',
  catchup=False):
None
```

Note : We can also define a DAG using 'dag = DAG('my_dag'...' but we use the context manager 'with' to avoid having to assign the DAG object to every task.

- The TaskFlow API :

```python
from airflow.decorators import dag
from datetime import datetime

@dag(start_date=datetime(2023, 1, 1), description='A simple tutorial DAG', 
     tags=['data_science'], schedule='@daily', catchup=False)
def my_dag():
    None

my_dag()
```

With the Taskflow API, instead of using the context manager with, you use the dag decorator.
The dag decorator expects a python function (here my_dag) where the name corresponds to your DAG id.
You will define/call tasks within this function.
Last but not least, do not forget to call your dag function ( here my_dag()) otherwise your DAG won't show up on the Airflow UI.

Now let's add our first task to the previously created DAG : 

```python
# add this import 
from airflow.operators.python import PythonOperator

# add the function you want to use
def print_a():
  print('hi from task a')

# replace None with your task in the DAG definition
  task_a = PythonOperator(task_id='task_a', python_callable=print_a)
```

With the TaskFlow API, we would write this :

```python
def my_dag():
    
    @task
    def print_a():
        print('hi from task a')
```

Instead of importing the PythonOperator, you import task decorator.
The TaskFlow API changes your way of writing DAGs. It becomes faster, cleaner and easier than with the traditional paradigm.

The last step is to define the order in which the tasks will be executed, that's what we need the dependencies for.
If you have 2 tasks, saying that task_b should be executed after task_a is as simple as writing this at the end of your DAG file :

```python
task_a >> task_b
```

Now let's say you have 5 tasks (a to e) and want to execute a first, then b to d at the same time, and then e :

```python
task_a >> [task_b, task_c, task_d] >> task_e
```

How about if we want to run b and c at the same time, and after that d and e at the same time ?
Airflow doesn't allow a dependency between two lists.
Instead, we can use a helper, in this case, a chain.

```python
from airflow.utils.helpers import chain

chain(task_a, [task_b, task_c], [task_d, task_e]) 
```

You can use the bitshift operators to define dependencies with the traditional paradigm.
It's no different with the TaskFlow API :

```python
def my_dag():

    @task
    def print_a():
        print('hi from task a')
    
...

    @task
    def print_e():
        print('hi from task e')


    print_a() >> print_b() >> print_c() >> print_d() >> print_e()

my_dag()
```
Notice that you must call your tasks as if they were usual python functions with the parentheses.

You have successfully created your first Airflow data pipeline !

Here is a complete DAG file for a pipeline to create a file using Bash and read it using Python :

```python
from airflow import DAG
from datetime import datetime
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

with DAG(dag_id='check_dag', schedule='@daily', 
        start_date=datetime(2023, 1, 1), catchup=False,
        description='DAG to check data', tags=['data_engineering']):
    
    create_file = BashOperator(
        task_id='create_file',
        bash_command='echo "Hi there!" >/tmp/dummy'
    )

    check_file_exists = BashOperator(
        task_id='check_file_exists',
        bash_command='test -f /tmp/dummy'
    )

    read_file = PythonOperator(
        task_id='read_file',
        python_callable=lambda: print(open('/tmp/dummy', 'rb').read())
    )

    create_file >> check_file_exists >> read_file
```
 
### DAG scheduling

Discover how your DAGs are scheduled in Airflow :
- What is a DAGRun
- How the start_date and schedule_interval work together
- back filling and catching up DAGs

#### DAG Run

Let's say you have a data pipeline with three tasks, as soon as the scheduler starts scheduling your DAG, you will end up with a DAG Run, which is defined by two values : data_interval_start and data_iterval_end.
a DAG Run goes through different states, first 'Queued', then 'Running' as soon as the first task starts, and finally 'Success' is the last task succeeds or 'Failure' otherwise.

#### DAG Scheduling

There 2 main parameteres for each DAG :
- start_date : the date at which the DAG starts being scheduled
(The real definition of start_date is "the timestamp from which the scheduler will attempt to backfill")

- schedule_interval : defines how often the DAG runs (every 10 minutes, once a month...)

You can configure your DAG for a basic schedule is by defining its schedule argument using either a cron expression or selecting one of the available cron "presets".

- None          Don’t schedule, use for exclusively “externally triggered” DAGs
- @once          Schedule once and only once
- @hourly          Run once an hour at the end of the hour
- @daily          Run once a day at midnight (24:00)
- @weekly          Run once a week at midnight (24:00) on Sunday
- @monthly          Run once a month at midnight (24:00) of the first day of the month
- @quarterly        Run once a quarter at midnight (24:00) on the first day
- @yearly          Run once a year at midnight (24:00) of January 1

You can also use a timedelta object to schedule a DAGs :

```python
from datetime import timedelta

# schedule your DAGs biweekly
schedule = timedelta(weeks=2),
```

#### The Catchup parameter and Backfill

On top of start_date and schedule_interval, you can set the parameter cathup to either true, Airflow will run any past scheduled intervals that have not been run, or false, and in this case only the latest non-triggered DAG Run will be executed. 

Similar to the catchup mechanism, the backfill mechanism allows you to run historical DAG Runs.
You can backfill using the Aiflow UI or the CLI using this command :

```
airflow backfill -s <START_DATE> -e <END_DATE> --rerun_failed_tasks -B <DAG_NAME>
```

or

```
airflow dags backfill --start-date START_DATE --end-date END_DATE dag_id
```

This will execute all DAG runs that were scheduled between START_DATE & END_DATE irrespective of the value of the catchup parameter in airflow.cfg.

Note : -B means we want DAG Runs to happen in backwards. Latest date first then the older dates.

### Connections 101

Airflow is meant to interact with various tools in your data stack.
To interact with those tools and services, you need to create a connection.
That's exactly what this module covers.

#### What is a connection ?

If you want to interact with an external system, like an API for loading / Snowflake for loading / dbt for transformation, you need to create a connection.
A connection is nothing more than a set of parameters (login, password, hostname...) with a unique connection ID. 

You can add a connection using the Airflow UI, by going to Admin > Connections.



### XComs 101

### Variables 101

### Debug DAGs

### Sensors

### Command Line Interface

### 
