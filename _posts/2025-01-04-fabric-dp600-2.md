---
layout: post
title: Microsoft Fabric - Analytics Engineer course (2/3)
categories: [Data Engineering, Azure]
---

This is part 2 of the DP600 exam preparation [course](https://www.youtube.com/watch?v=Bjk93hi21QM).
The second part is about the largest of section of the exam, about preparing and serving data.
Here are my notes from today.

## Chapter 4 : Getting data into Fabric

This is the heart of the exam (40-45% of the final score)

Goals :
- Ingest data
- Copy data
- Create and manage shortcuts

### Ingest data

How can you bring data in to Fabric ?

- Data ingestion (ELT/ETL) : Dataflow, Data pipeline, Notebook, Eventstream

#### Dataflow
Dataflow has +150 connectors to external systems to bring data into a familiar Power Query low/no-code interface, perform data transformation and write to Fabric data stores.

Pros : +150 connectors, No/low-code, Can do Extract, Transform AND Load, accessing on-premise data (OPDG), you can combine more than one dataset at a time, can upload raw local files (csv file...)

Cons : can struggle with large datasets (Fast Copy has recently been introduced to speed things up), difficult to implement data validation, can't pass in external parameters

#### Data pipeline
Primarily an orchestration tool. Can be used to get data into Fabric using the Copy Data activity for example.

Pros : large datasets, importing cloud data, control over flow logic (looping over tables...), triggering a wide variety of actions (stored procedure...)

Cons : can't do the Transform natively (but can embed notebooks or dataflow), no ability to upload local files, does not work cross-workspace

#### Notebook
General purpose coding notebook which can be used to bring data into Fabric, via connecting to APIs or by using client Python libraries

Pros : extraction from APIs (using Python requests library), use of client libraries (Azure libraries, Hubspot client library in Python...), good for code re-use (and can be parameterised), data validation and data quality testing of incoming data, the fastest and most efficient in terms of data ingestion

Cons : need Python knowledge


You can also ingest data through shortcuts.

- Shortcuts (internal and external) : Lakehouse tables/files, Warehouse tables, KQL tables

A shortcut enables you to create a live link to data stored eitheri in another part of Fabric (internal shortcut) or in external storage locations like : ADLS Gen2, Amazon S3 or other S3 services, Google Cloud Storage, Dataverse

Be careful with cross-region egress fees. If you Fabric capacity is in the UK-South region, but your ADLS storage account is in West-US, you will be charged a 'cross-region' fee.

OneLake shortcuts can now be created using the Fabric REST API.

The third and last way to ingest the data is through mirroring :

- Database mirroring : Snowflake, CosmosDB, Azure SQL

This part is not included in the test.

## Chapter 5 : SQL, Data Warehouse and Scheduling

Goals : 
- Create views, functions and stored procedures
- Add stored procedures, notebooks and dataflows to a data pipeline
- Schedule data pipelines
- Schedule dataflows and notebooks

### Creating functions, stored procedures and views

You can connect externally to your datawarehouse/datalake, for example through Microsoft SQL Server Management Studio, buy going to Settings > SQL analytics endpoint and copying the SQL connection string.
Or you can use the SQL endpoint directly in Fabric.

There you can create functions.

Functions are useful for packaging up logic, which can be parameterized (one or many parameters).
It's only for SELECT statements, you can't edit the data by using UPDATE/INSERT etc...

Functions can be called from DDL statements or from within Stored procedures or a view.
```sql
-- A function that takes a first name as input
-- and filter the Employees table

CREATE FUNCTION dbo.fn_Employee_FirstNameSearch ( @firstname varchar(20) = '' )
RETURNS TABLE 
AS 
RETURN
(
	SELECT
		EmployeeId,
		Name,
		Age
	from dbo.Employees
	WHERE Name like @firstname + '%'
);

-- Test the function
SELECT * FROM dbo.fn_Employee_FirstNameSearch('Mourad');
```

Stores procedures on the other hand, are called using EXEC (not as part of a select statement)
They give the ability to define input AND output parameters.
Unlike functions, stored procedures have the ability to do INSERTS, UPDATES, DELETES...
SP can be embedded in a data pipeline, so they're more useful for data transformation for example.

```sql
CREATE PROCEDURE dbo.sp_Employee_getByFirstName @firstname varchar(20)
AS
SELECT * FROM dbo.Employees
WHERE Name LIKE @firstname + '%';

EXEC dbo.sp_Employee_GetByFirstName 'Jack';
```

### Add stored procedures, notebooks and dataflows to a data pipeline

You can use previously created Stored procedures in Fabric Data pipeline.
Only SP in datawarehouses are available, not lakehouses.

### Scheduling

There are different ways to schedule things to work in Fabric.

For dataflows, we can schedule them directly from the workspace through the 3 dots of a dataflow > Settings > Refresh > Configure a refresh schedule.

For Notebooks it can be done through the workspace as well, for a specific notebook click the 3 dots > Schedule and activate Scheduled run.

For a data pipeline you have to open it, there you have a Schedule button.

## Chapter 6 : Transform Data

### Data cleansing process

Data cleansing inclues cleaning duplicate data/missing data/null values, converting data types and filtering data.
In a medallion architecture, the cleansing takes place mostly between the bronze and silver layers.
The most common tools to use are T-SQL, Spark and Dataflow.

Power Query allows you to automatically delete duplicate rows by selecting all column and double clicking.

You can also use T-SQL using the SQL endpoint of a datawarehouse :

```sql
-- identifying duplicates

SELECT id, date, count(revenue)
FROM revenue
GROUP BY id, date
HAVING count(revenue) > 1;

-- deduplication
SELECT id, date, max(revenue)
FROM revenue
GROUP BY id, date;

-- identifyin/removing nulls

SELECT id, model, branch, date
FROM revenue 
WHERE revenue IS NOT NULL;

-- Type conversion and adding new columns

SELECT id, 
	revenue,
	CAST(revenue AS FLOAT) as revFloat,
	revenue/2 as HalfRevenue
FROM revenue;


```

Finally you can transform data using PySpark notebooks.

```python
df = spark.sql("SELECT * FROM LH_BRONZE.revenue")
display(df)

# identifying duplicates
df.groupby(['Branch_ID', 'Date_ID']).count().where('count > 1').show()

df.count()
deduped = df.dropDuplicates()
deduped.count()

print(f"Process removed {df.count() - deduped.count()} rows from the dataset")

# identifyin missing data
# option 1 : using isNull()

nulls = df.filter(df.Revenue.isNull())
display(nulls)

# option 2 : using where and col
from pyspak.sql.function import col
nulls2 = df.where(col("Revenue").isNull())
display(nulls2)

# dropping null values

no_nas = df.dropna(subset=['Revenue'])
print(f"Process removed {df.count() - no_nas.count()} rows from the dataset")

# Type conversion + Adding new columns
df.printSchema()

type_conv = df.withColumn('UnitsSoldConverted', df.Units_Sold.cast("string"))
type_conv.printSchema()
print(type_conv)

# Filter data
filtered_df = df.where(col("Revenue") > 1000000)

# Data enrichement
enriched_df = df.withColumn('halfRevenue', df.Revenue/2)
display(enriched_df)

# Joining and merging

dealers_df = spark.sql("SELECT * FROM dealers")
countries_df = spark.sql("SELECT * FROM countries")

joined_df = dealers_df.join(countries_df, dealers_df.country_id == countries.country_id).select(dealers_df.dealer_id, dealers_df.country_id, countries_df.country_name)

display(joined_df)

```

### Data cleansing

Modeling is typically done in the gold layer.

The most common model is the star schema: a fact table in the middle (car sales for example) and linked to it are multiple dimension tables through primary keys (for example branch, dealers, model, date)

Usually the fact table is append only (adding new sales) we shouldn't be updating it.
The dimension tables can change over time (details about a particular branch might change), you can deal with those changes with slowly changing dimensions (SCD) :

- Type 1 SCD is OVERWRITE writing mode frorm either a data piepline, dataflow or using pySpark. 

- Type 2 SCD needs 3 new columns to be implemented : ValidFrom, ValidTo and IsCurrent flag
If you're using Lakehouse/Spark : delta logs storea history of all writes, so this might be a simpler option.
If not you can use the MERGE INTO to updating in Delta

If you're using a Data Warehouse / T-SQL : load to staging, then stored procedures to perform inserts, updates and validfrom, validto calculations.
Merge operation in Fabric Data Warehouse is currently not supported 
Can implement row hashing to check for changes to rows : hash the value of an entire row both in the existing data set and new data set that you're checking, if the hash values match, the record haven't changed, if not you can go ahead with the update

Another common model is the Normalized data model (snowflake)
It's similar to the star schema, but with an extra level of dimensions for example (dim_cities connected to dim_dealer) 
If we merge dim_cities and dim_dealer, it becomes a denormalized data model.

### Data aggregation

DA is transforming a data set from a more granular data set to a less granular one (aggregating revenue by country for example)
Typically implemented using GROUP BY (Dataflow, T-SQL or Spark)
Type of aggregation varies depending on use case (COUNT, SUM, MAX, MIN, AVG...)

## Chapter 7 : Optimize performance

Goals : 
- Identify and resolve data loading performance bottlenecks 
- Implement performance improvements
- Identify and resolve issues with Delta table file sizes
- Implement file partitioning


|                    | Identify performance issues                                               | Possible resolutions                     |
| ------------------ | ------------------------------------------------------------------------- | ---------------------------------------- |
| Dataflow           | Refresh history<br>Monitoring hub<br>Capacity metrics app                 | Refactoring<br>Staging<br>Fast Copy      |
| SQL Data Warehouse | Query insights<br>DMVs<br>Capacity Metrics App<br>Statistics & query plan | Refactoring<br>Query optimization        |
| Notebook (Spark)   | Spark history server<br>Monitoring hub<br>Capacity metrics app            | Refactoring<br>Query optimization        |
| Delta files        | DESCRIBE                                                                  | V-Oder optimization<br>File partitioning |

### General tools for performance monitoring

- Monitoring Hub : you can open it by clicking on Monitor on the left tool bar
- Capacity metrics app : a Power BI app that you can download on the Apps area (left tool bar)
- Dynamic Management Views (DMVs) : can be found in sys > Views and provide detailed insight when runinng SQL, like who is the user, when was the session started, how many queries are active, which queries are long running...
- Query insights : can be found in queryinsights > views, a more user friendly abstraction of DMVs

### Performance optimization features 

- Staging : by default, transformations in a dataflow are carried out by the PowerQuery engine (Mashup engine) that is relatively slow compared to the Spark engine. 
Staging involves getting the Mashup engine to get the data from the source, write it to a Staging Lakehouse, perform the transformation using the Spark engine, then the Mashup engine reads the result and writes the data to destination. 
You can enable staging by right clicking the query and selecting "Enable staging" option.
- Fast Copy : a new feature to speed up getting new data from sources 
- Partitioning : choosing the right partition column (usually a date). You shouldn't use a column with high cardinality (user id). The amount of data in each partition should be around 1 Gb.

```python
# Implementing partitioning 

df = spark.read.parquet("Files/flights.parquet")
display(df)

from pyspark.sql.functions import col, year, month, quarter, dayofmonth

# prepare our dataset for partitioning
transformed_df = df.withColumns({'year':year(df.FL_DATE), 'month':month(df.FL_DATE), 'day':dayofmonth(df.FL_DATE)})
display(transformed_df)

# write without partitions
transformed_df.write.mode("overwrite").format("delta").save("Tables/not_partitioned")

# write with partitions
transformed_df.write.mode("overwrite").format("delta").partitionBy("year", "month").save("Tables/partitioned")

# write with more partitions
transformed_df.write.mode("overwrite").format("delta").partitionBy("year", "month", "day").save("Tables/partitioned_daily")
```

- V-Oder optimization : a Microsoft proprietary algorithm that provides sorting/compaction of Parquet files. It is enabled by default.

```python
spark.conf.get('spark.sql.parquet.vorder.enabled')

# to disable it
spark.conf.set('spark.sql.parquet.vorder.enabled', false)
```

- Delta table ptimization : OPTIMIZE (a delata lake method that performs bin-compaction), VACUUM (removal of files no longer referenced)
- Spark optimization : coalesce (reduces the amount of partitions in a delta table, it doesn't require a shuffle so it's efficient), repartition (breaking of existing partitions to create new ones, it's expensive because it requires shuffling)

- See you tomorrow for the third and last part of this course !
